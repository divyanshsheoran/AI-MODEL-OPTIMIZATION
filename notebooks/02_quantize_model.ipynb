{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e618a936-d425-43ef-a28c-cf86c879a7ca",
      "metadata": {
        "id": "e618a936-d425-43ef-a28c-cf86c879a7ca"
      },
      "source": [
        "# ðŸ”§ 02 - Quantize MobileNetV2 with PyTorch\n",
        "\n",
        "This notebook applies **post-training static quantization** to a MobileNetV2 model trained on CIFAR-10.  \n",
        "It compares accuracy and model size before and after quantization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "71f857f6-a043-4296-b175-2e040047458a",
      "metadata": {
        "id": "71f857f6-a043-4296-b175-2e040047458a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.quantization\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-aiqtgJ3VcH",
        "outputId": "1219afbe-4f72-43c7-9972-2d75ebf18d97"
      },
      "id": "L-aiqtgJ3VcH",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "7661f6af-2f8e-4c41-89e3-9a29fb20b77e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7661f6af-2f8e-4c41-89e3-9a29fb20b77e",
        "outputId": "434c4fd4-b4b6-4028-b15f-8e8450a1981c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Supported quantization engines: ['qnnpack', 'none', 'onednn', 'x86', 'fbgemm']\n",
            "Using device: cpu | Quantization engine: fbgemm\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")  # Quantization only works on CPU\n",
        "\n",
        "# Set quantization engine for Windows CPU\n",
        "import torch.backends.quantized\n",
        "print(\"Supported quantization engines:\", torch.backends.quantized.supported_engines)\n",
        "\n",
        "torch.backends.quantized.engine = 'fbgemm'  # fbgemm is supported on Windows CPU\n",
        "\n",
        "print(f\"Using device: {device} | Quantization engine: {torch.backends.quantized.engine}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "12405831-2df0-48b3-b18a-0ddfce331982",
      "metadata": {
        "id": "12405831-2df0-48b3-b18a-0ddfce331982"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='../data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d7858c20-ca4a-4f0d-936e-51fa865924fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7858c20-ca4a-4f0d-936e-51fa865924fd",
        "outputId": "b3a3479f-dd4f-4e95-fb3f-82cc4c6c051e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loaded baseline FP32 model.\n"
          ]
        }
      ],
      "source": [
        "model_fp32 = models.mobilenet_v2(pretrained=False)\n",
        "model_fp32.classifier[1] = nn.Linear(model_fp32.last_channel, 10)\n",
        "model_fp32.load_state_dict(torch.load(\n",
        "    \"/content/drive/MyDrive/AI_MODEL_OPTIMIZATION/models/mobilenetv2_cifar10_baseline.pth\",\n",
        "    map_location=device\n",
        "))\n",
        "\n",
        "model_fp32.eval()\n",
        "print(\" Loaded baseline FP32 model.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "abd944e1-1f00-4434-943d-c4cba56d581a",
      "metadata": {
        "id": "abd944e1-1f00-4434-943d-c4cba56d581a"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            # Ensure the model is on the correct device\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "8f1a9e7a-ddec-4066-a5c7-66013cda6bd4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f1a9e7a-ddec-4066-a5c7-66013cda6bd4",
        "outputId": "186e37bd-e7eb-4a9b-86d4-7617f82c7177"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Baseline FP32 Accuracy: 58.01%\n"
          ]
        }
      ],
      "source": [
        "acc_fp32 = evaluate(model_fp32, testloader, device)\n",
        "print(f\" Baseline FP32 Accuracy: {acc_fp32:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "c4bce3e4-05fa-497e-b945-6ccea529f0cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4bce3e4-05fa-497e-b945-6ccea529f0cc",
        "outputId": "dffc7ee4-c543-4e67-b34e-a004b15ff2ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Model dynamically quantized.\n"
          ]
        }
      ],
      "source": [
        "import torch.quantization\n",
        "\n",
        "model_to_quantize = models.mobilenet_v2(pretrained=False)\n",
        "model_to_quantize.classifier[1] = nn.Linear(model_to_quantize.last_channel, 10)\n",
        "model_to_quantize.load_state_dict(torch.load(\n",
        "    \"/content/drive/MyDrive/AI_MODEL_OPTIMIZATION/models/mobilenetv2_cifar10_baseline.pth\",\n",
        "    map_location=device\n",
        "))\n",
        "model_to_quantize.eval()\n",
        "\n",
        "# Apply dynamic quantization\n",
        "model_dynamic_quantized = torch.quantization.quantize_dynamic(\n",
        "    model_to_quantize,  # the original model\n",
        "    {nn.Linear},        # layers to quantize\n",
        "    dtype=torch.qint8   # quantized dtype\n",
        ")\n",
        "\n",
        "print(\" Model dynamically quantized.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "1bcbc8c9-03cd-4cdb-95aa-78159b025747",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bcbc8c9-03cd-4cdb-95aa-78159b025747",
        "outputId": "0808687c-7086-40cc-8448-4c2d0501e6b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Quantized INT8 Accuracy: 58.01%\n"
          ]
        }
      ],
      "source": [
        "acc_int8 = evaluate(model_to_quantize, testloader, device)\n",
        "print(f\" Quantized INT8 Accuracy: {acc_int8:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "e2f32b1b-425c-4c98-ad13-4f0b01b979e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2f32b1b-425c-4c98-ad13-4f0b01b979e5",
        "outputId": "ac7981d3-14d8-485b-82be-458b00bf1782"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved to '../models/mobilenetv2_quantized.pth'\n"
          ]
        }
      ],
      "source": [
        "os.makedirs(\"../models\", exist_ok=True)\n",
        "torch.save(model_to_quantize.state_dict(), \"../models/mobilenetv2_quantized.pth\")\n",
        "print(\" Saved to '../models/mobilenetv2_quantized.pth'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "9b2b3f91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b2b3f91",
        "outputId": "c7fbbf42-be64-4eb8-da91-1070d1ed3407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " FP32 Model Size: 9.19 MB\n",
            " INT8 Model Size: 9.19 MB\n",
            " Size Reduction: 0.03%\n"
          ]
        }
      ],
      "source": [
        "size_fp32 = os.path.getsize(\"/content/drive/MyDrive/AI_MODEL_OPTIMIZATION/models/mobilenetv2_cifar10_baseline.pth\") / 1e6\n",
        "size_int8 = os.path.getsize(\"../models/mobilenetv2_quantized.pth\") / 1e6\n",
        "\n",
        "print(f\" FP32 Model Size: {size_fp32:.2f} MB\")\n",
        "print(f\" INT8 Model Size: {size_int8:.2f} MB\")\n",
        "print(f\" Size Reduction: {(size_fp32 - size_int8) / size_fp32 * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}